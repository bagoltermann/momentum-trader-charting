# Session Notes - 2026-01-28

## Status
Warrior Trading chart enhancements (Gap %, Volume, D1 High breakout alert) + 8 stability fixes. All changes implemented and pushed to GitHub.

## Session Overview
1. Warrior Trading chart enhancements - Gap %, pre-market volume, D1 High breakout alert
2. Streaming relay disconnect fallback - Frontend falls back to REST polling when trader app disconnects
3. Zero candle data crash fix - Filter out placeholder candles with all-zero OHLC
4. httpx request hang band-aid fix - asyncio.wait_for() wrapper around API calls
5. Thread pool exhaustion fix - Increased thread pool to 50 workers, added timeout wrappers
6. Frontend crash prevention - ErrorBoundary, timestamp tracking for incremental updates
7. Async httpx client conversion - Replaced sync httpx with native async in file_watcher.py
8. Launcher cleanup improvement - Kill by port first to handle hung backend processes

## Work Completed

### Feature 1: Gap % + Volume Display (Warrior Trading Metrics)
**Files**: `src/renderer/components/charts/EnhancedChart.tsx`, `src/renderer/components/charts/MultiChartGrid.tsx`, `src/renderer/styles/global.css`
**Risk**: LOW

Added Warrior Trading metrics to chart header:
- **Gap % badge**: Shows Day 1 gap percentage from runner data
  - Green (strong): >= 20%
  - Yellow (moderate): 10-20%
  - Gray (weak): < 10%
- **Volume badge**: Shows total pre-market volume with ratio to average
  - Green (strong): >= 2x average
  - Yellow (moderate): 1-2x average
  - Gray (weak): < 1x average

**Props added to EnhancedChart**:
```typescript
gapPercent?: number        // Day 1 gap percentage from runner data
totalVolume?: number       // Sum of candle volumes (pre-market volume)
avgVolume?: number         // Average daily volume for ratio calculation
```

**Calculations in MultiChartGrid**:
```typescript
const primaryGapPercent = useMemo(() => {
  if (!primarySymbol) return undefined
  const runner = runners.find(r => r.symbol === primarySymbol)
  return runner?.original_gap_percent
}, [primarySymbol, runners])

const primaryTotalVolume = useMemo(() => {
  if (primaryCandles.length === 0) return undefined
  return primaryCandles.reduce((sum, c) => sum + (c.volume || 0), 0)
}, [primaryCandles])

const primaryAvgVolume = useMemo(() => {
  if (!primarySymbol) return undefined
  const runner = runners.find(r => r.symbol === primarySymbol)
  return runner?.day1_volume
}, [primarySymbol, runners])
```

### Feature 2: D1 High Breakout Alert
**File**: `src/renderer/components/charts/EnhancedChart.tsx`
**Risk**: LOW

Dynamic styling for D1 High price line based on proximity to current price:

| State | Distance | Line Style | Color | Width | Label |
|-------|----------|------------|-------|-------|-------|
| Normal | > 2% below | Dashed | #00E676 | 1 | D1 High |
| Approaching | 0-2% below | Solid | #FFD600 | 2 | D1 High (approaching) |
| Breakout | Above | Solid | #00E676 | 3 | D1 High (BREAKOUT) |

**Implementation**:
```typescript
if (zone.label === 'D1 High' && currentPrice && zone.price > 0) {
  const pctFromD1High = ((zone.price - currentPrice) / zone.price) * 100

  if (pctFromD1High < 0) {
    // BREAKOUT - price is ABOVE D1 High
    color = '#00E676'; lineWidth = 3; lineStyle = 0
    title = 'D1 High (BREAKOUT)'
  } else if (pctFromD1High < 2) {
    // APPROACHING - within 2% of D1 High
    color = '#FFD600'; lineWidth = 2; lineStyle = 0
    title = 'D1 High (approaching)'
  }
}
```

### Fix 1: Streaming Relay Disconnect Fallback
**Files**: `backend/services/quote_relay.py`, `src/renderer/hooks/useStreamingQuotes.ts`
**Risk**: LOW

**Problem**: When trader app crashes or is stopped while charting app is streaming, charts go blank. Frontend WebSocket to backend stays open (no error), but no data flows. Frontend thinks `streamingConnected = true`, so REST polling stays paused.

**Two-layer fix**:
1. **Backend relay status notifications**: QuoteRelay now sends `{type: 'status', connected: bool}` messages to all WebSocket clients when trader app connects/disconnects
2. **Frontend stale timeout**: If no quotes received for 60s, sets `streamingConnected = false` to trigger REST polling fallback

**Backend changes (quote_relay.py)**:
```python
def add_status_callback(self, cb):
    """Register callback for connection status changes"""
    self._status_callbacks.append(cb)
    # Immediately notify of current status
    cb({'type': 'status', 'connected': self._connected})

def _notify_status(self, connected: bool):
    """Notify all status callbacks of connection change"""
    for cb in self._status_callbacks:
        cb({'type': 'status', 'connected': connected})

def _on_connect(self):
    self._connected = True
    self._notify_status(True)

def _on_disconnect(self):
    self._connected = False
    self._notify_status(False)
```

### Fix 2: Zero Candle Data Crash
**File**: `src/renderer/hooks/useCandleData.ts`
**Risk**: LOW

**Problem**: Clicking on symbols during pre-market hours when Schwab API returns placeholder candles with OHLC = 0 causes chart to go blank or crash.

**Fix**: Filter out candles where OHLC are all zero during transform. Show "No trades yet" message instead of crashing.

```typescript
// Skip candles where OHLC are all zero (invalid/placeholder data)
if (c.open === 0 && c.high === 0 && c.low === 0 && c.close === 0) {
  debugLog(`[useCandleData] Skipping zero candle at ${c.timestamp}`)
  continue
}

// Handle case where all candles were filtered out
if (transformed.length === 0 && response.data.length > 0) {
  debugLog(`[useCandleData] All candles filtered (zeros) for ${symbol}`)
  if (mountedRef.current) {
    setError('No trades yet')
  }
  return
}
```

### Fix 3: httpx Request Hang (Band-Aid)
**File**: `backend/services/schwab_client.py`
**Risk**: LOW

**Problem**: Rapid clicking through watchlist stocks during pre-market causes backend to become unresponsive. httpx's internal timeout wasn't firing reliably.

**Band-aid fix**: Added `asyncio.wait_for()` wrapper around all `make_api_request()` calls with hard timeouts:
- 15s for price history requests
- 10s for quote requests

```python
response = await asyncio.wait_for(
    make_api_request(...),
    timeout=15.0  # Hard timeout
)
```

**Root cause unknown** - possible causes documented in troubleshooting guide:
1. Connection pool exhaustion
2. SSL handshake hang
3. DNS resolution hang
4. httpx Windows-specific bug
5. Schwab API rate limiting silently dropping connections

See `docs/session-notes/2026-01-27-chart-not-displaying-troubleshooting.md` failure mode #8 for full analysis.

## Testing Checklist
- [x] Gap % badge displays for runner stocks
- [x] Volume badge displays with ratio when avgVolume available
- [x] D1 High line changes style based on price proximity
- [x] Streaming disconnect triggers REST polling fallback within 60s
- [x] Zero candle data shows "No trades yet" instead of crash
- [x] Rapid symbol switching doesn't hang backend (timeouts fire)

## Files Modified

| File | Description |
|------|-------------|
| `src/renderer/components/charts/EnhancedChart.tsx` | Added gapPercent/volume props, badges in header, D1 High proximity logic |
| `src/renderer/components/charts/MultiChartGrid.tsx` | Calculate and pass gap%, volume, avgVolume to EnhancedChart |
| `src/renderer/styles/global.css` | Added gap-badge and volume-badge styles |
| `src/renderer/hooks/useCandleData.ts` | Zero candle filtering |
| `src/renderer/hooks/useStreamingQuotes.ts` | NEW - WebSocket client with stale detection |
| `backend/services/quote_relay.py` | Status callbacks for connect/disconnect events |
| `backend/services/schwab_client.py` | asyncio.wait_for() timeout wrappers |
| `docs/session-notes/2026-01-27-chart-not-displaying-troubleshooting.md` | Updated failure mode #8 with band-aid analysis |

## Key Takeaways
- **Pre-market data is sparse** - Many small-cap runner stocks have zero trades before market open. Schwab returns placeholder candles with all zeros. Filtering these is essential to prevent chart crashes.
- **asyncio.wait_for() is a valid defensive pattern** - Even when you trust a library's internal timeout, wrapping with `asyncio.wait_for()` provides a guaranteed hard cutoff. It's production-grade, not just a hack.
- **Streaming fallback needs multiple layers** - Backend status notifications handle immediate disconnect detection, but a stale timeout handles cases where the status message is missed (network glitch, etc.).
- **D1 High proximity is valuable** - Knowing when price is approaching the breakout level (within 2%) lets the trader prepare rather than react.

### Fix 4: Sync Watchlist Fetch Blocking Event Loop (ROOT CAUSE FOUND)
**Files**: `backend/services/file_watcher.py`, `backend/api/routes.py`
**Risk**: LOW

**Problem**: Backend hung after ~2 hours of operation. Log showed watchlist polls stopping (last entry was a successful watchlist fetch), then complete silence - no incomplete Schwab API requests.

**Root cause analysis**:
- The previous httpx hang theory was incorrect for this occurrence
- `file_watcher.py` used **synchronous httpx** (`httpx.Client`) to fetch watchlist from trader app
- Even with a 5s timeout, the sync call blocked the entire async event loop while waiting
- If trader app became slow (busy processing), the blocking call froze all other requests
- This explains why the log showed successful watchlist fetches then silence - the next watchlist fetch blocked forever

**Fix**:
1. Created `get_cached_watchlist_async()` wrapper using `asyncio.to_thread()`
2. Updated `/api/watchlist` route to use the async version
3. Kept original sync version for startup initialization

```python
async def get_cached_watchlist_async(refresh: bool = False) -> Optional[List[Dict]]:
    if refresh:
        if time_module.time() - _watchlist_cache_time > _WATCHLIST_CACHE_TTL:
            await asyncio.to_thread(fetch_watchlist_from_trader)
    with _cache_lock:
        return _cached_watchlist
```

### Fix 5: Missed Call Site in /validate Route
**Files**: `backend/api/routes.py`
**Risk**: LOW

**Problem**: After first fix, backend still hung immediately on first validation request. Log showed "POST /validate/OM started" then silence.

**Root cause**: The `/validate/{symbol}` route on line 180 also called `get_cached_watchlist()` but we only updated the `/api/watchlist` route. Since we changed the import to only include `get_cached_watchlist_async`, the old call raised a `NameError`.

**Fix**: Changed line 180 to use `await get_cached_watchlist_async()`.

### Fix 6: Thread Pool Exhaustion from LLM Validations
**Files**: `backend/main.py`, `backend/services/llm_validator.py`, `backend/services/file_watcher.py`, `backend/api/routes.py`
**Commit**: `0dc9a5d`
**Risk**: LOW

**Problem**: Backend hung despite sync watchlist fix. Multiple concurrent LLM validation requests consumed all threads in the default executor pool (20 threads). Even simple operations couldn't get a thread.

**Root cause chain**:
1. `asyncio.to_thread()` uses a shared default executor (ThreadPoolExecutor)
2. Default pool size on Windows is `min(32, cpu_count + 4)` = 20 threads
3. LLM validation calls `requests.post()` with 60s timeout, blocking a thread
4. Top 3 validation (3 symbols × multiple `to_thread` calls) can consume 9+ threads
5. Once pool exhausted, even watchlist fetch blocks waiting for a thread

**Fix (multi-layer)**:
1. Increased thread pool to 50 workers in `main.py`
2. Added `asyncio.wait_for()` timeout wrappers around all `to_thread()` calls
3. Graceful fallback on timeout - returns cached/fallback data

```python
_thread_pool = ThreadPoolExecutor(max_workers=50, thread_name_prefix="asyncio_pool")
loop.set_default_executor(_thread_pool)
```

### Fix 7: Frontend Crash Prevention and Chart Error Handling
**Files**: `src/renderer/components/charts/EnhancedChart.tsx`, `src/renderer/components/charts/MultiChartGrid.tsx`, `src/renderer/components/ErrorBoundary.tsx`
**Commit**: `46b5770`
**Risk**: LOW

**Problem**: Frontend freeze/blank screen when clicking stocks with sparse data. Uncaught exceptions in chart operations crashed the React component tree. Also: lightweight-charts "Cannot update oldest data" error when incremental `update()` called with older timestamp.

**Fix (multi-layer)**:
1. **ErrorBoundary component**: Catches React rendering errors, shows "Try Again" button
2. **Defensive indicator calculations**: Try-catch around VWAP, EMA9, EMA20
3. **Error handling in chart effects**: Try-catch around all three useEffect hooks
4. **Timestamp tracking**: Track `lastTime` to prevent `update()` with older data

```typescript
const prevDataRef = useRef<{ count: number; symbol: string; lastTime: number }>({ count: 0, symbol: '', lastTime: 0 })

// Only use update() when timestamp is newer
const isIncremental = prev.symbol === symbol &&
  prev.count > 0 &&
  candles.length <= prev.count + 1 &&
  lastCandleTime >= prev.lastTime  // NEW: timestamp check
```

### Fix 8: Replace Sync httpx Client with Async Client
**Files**: `backend/services/file_watcher.py`, `backend/main.py`
**Commit**: `914e297`
**Risk**: LOW

**Problem**: Backend still hung despite thread pool fix. The module-level sync `httpx.Client` could get stuck indefinitely even when wrapped in `asyncio.to_thread()`.

**Root cause**: Sync httpx.Client:
1. Had no connection limits (could exhaust connections)
2. The 5.0s timeout only applies to request operations, not connection/SSL establishment
3. If trader app became slow, the sync client could hang waiting for connections
4. This consumed thread pool threads indefinitely

**Fix**:
1. Converted to native `httpx.AsyncClient` with proper limits
2. Native async fetch - no thread pool needed
3. Hard timeout wrapper with `asyncio.wait_for()`
4. Startup sync uses one-off context manager

```python
_async_httpx_client = httpx.AsyncClient(
    timeout=httpx.Timeout(5.0, connect=3.0),
    limits=httpx.Limits(max_connections=5, max_keepalive_connections=2)
)

async def fetch_watchlist_from_trader_async():
    client = await _get_async_httpx_client()
    response = await asyncio.wait_for(
        client.get(f"{_trader_api_url}/api/watchlist"),
        timeout=5.0
    )
```

### Fix 9: Launcher Cleanup for Hung Backend
**Files**: `launcher.py`
**Commit**: `3ad3caf`
**Risk**: LOW

**Problem**: When backend hangs and user closes app, the old hung process persists blocking port 8081. Next launch connects to zombie process.

**Root cause**: Launcher cleanup relied on `/api/shutdown` API (times out if hung) and PID kill (but uvicorn may spawn child with different PID).

**Fix**:
1. Kill processes by port FIRST before graceful shutdown
2. Double-pass cleanup (start and end of routine)
3. Better logging of PIDs killed

```python
def cleanup(self):
    # FIRST: Kill anything on our ports
    if self.platform == 'Windows':
        self._kill_port_holders([8081, 5173])
        time.sleep(0.3)

    # Then try graceful shutdown...

    # Final safety pass
    if self.platform == 'Windows':
        self._kill_port_holders([8081, 5173])
```

## Files Modified (Additional)

| File | Description | Commit |
|------|-------------|--------|
| `backend/main.py` | Thread pool 50 workers, cleanup async client | `0dc9a5d`, `914e297` |
| `backend/services/llm_validator.py` | Timeout wrappers around to_thread calls | `0dc9a5d` |
| `backend/services/file_watcher.py` | Native async httpx client | `914e297` |
| `src/renderer/components/ErrorBoundary.tsx` | NEW - React error boundary | `46b5770` |
| `src/renderer/components/charts/EnhancedChart.tsx` | Error handling, timestamp tracking | `46b5770` |
| `launcher.py` | Port-first cleanup for hung processes | `3ad3caf` |
| `docs/session-notes/2026-01-27-chart-not-displaying-troubleshooting.md` | Failure modes #10-#13 | Multiple |

### Fix 10: Remove asyncio.Lock from httpx Client Management
**Files**: `backend/services/schwab_client.py`, `backend/services/file_watcher.py`, `backend/main.py`
**Risk**: LOW

**Problem**: Backend still hangs despite all previous fixes. Analysis shows:
- Massive CLOSE_WAIT connection buildup (20+ connections)
- Log stops mid-operation with no errors
- Event loop completely frozen (no heartbeat)

**Root cause**: Lazy `asyncio.Lock()` initialization combined with lock contention:
1. `_get_lock()` creates lock lazily on first access
2. Lock created outside proper event loop context on Windows
3. If one coroutine holds lock during httpx issue, all others queue up
4. Can cause deadlock if httpx internal state becomes corrupted

**Fix**:
1. Remove asyncio.Lock from client management - not needed for single-assignment
2. Disable HTTP/2 in httpx - can cause issues on Windows long-running connections
3. Simplify to lock-free pattern: check-then-set (worst case: create two clients, one GC'd)
4. Enhanced heartbeat to log pending task count for debugging

```python
# Before (problematic):
async with _get_lock():
    if _shared_client is None or _shared_client.is_closed:
        _shared_client = httpx.AsyncClient(...)

# After (lock-free):
if _shared_client is not None and not _shared_client.is_closed:
    return _shared_client
_shared_client = httpx.AsyncClient(..., http2=False)
```

### Fix 11: Fresh httpx Client Per Request (Connection Pool Corruption)
**Files**: `backend/services/schwab_client.py`, `backend/services/file_watcher.py`
**Risk**: LOW (less efficient but more robust)

**Problem**: Backend still hangs despite Fix #10. Analysis shows:
- Logs stop mid-operation with no errors (same pattern as before)
- ESTABLISHED connection to Schwab API remains open
- httpx connection pool corruption suspected

**Root cause hypothesis**: httpx's connection pool can get into a corrupted state on Windows:
1. HTTP/1.1 keep-alive connections can become stale
2. SSL/TLS handshake timeouts may not be properly handled
3. Corrupted connection stays in pool, blocking future requests
4. All coroutines waiting for healthy connection freeze

**Fix**:
1. Abandon shared httpx client entirely
2. Create fresh `async with httpx.AsyncClient()` per request
3. Each request creates, uses, and closes its own client
4. No connection pool = no pool corruption possible

```python
# Before (shared client with pool):
client = await _get_client()  # may return corrupted connection
response = await client.get(url)

# After (fresh client per request):
async with httpx.AsyncClient(timeout=..., http2=False) as client:
    response = await client.get(url)
    # client closed automatically on exit
```

**Trade-off**: Slightly higher latency per request (~50ms for TLS handshake), but eliminates hang risk entirely.

### Fix 12: threading.Lock Blocking Event Loop in file_watcher.py
**Files**: `backend/services/file_watcher.py`, `backend/main.py`
**Risk**: LOW

**Problem**: Backend still hangs despite fresh client per request (Fix #11). Same pattern: log stops, CLOSE_WAIT buildup, event loop frozen.

**Root cause**: `threading.Lock` (`_cache_lock`) used in async code blocks the entire event loop:
1. File watcher thread (watchdog) calls `_reload_runners()`, acquires `_cache_lock`
2. Async route calls `get_cached_watchlist_async()`, reaches `with _cache_lock:`
3. Event loop thread blocks waiting for the threading.Lock
4. All async operations freeze until file watcher releases lock

**Key insight**: `threading.Lock` in async code is dangerous - it blocks the event loop thread. Even if held briefly, any contention causes the entire server to freeze.

**Fix**:
1. Remove `_cache_lock` from all read operations (reads are atomic in Python due to GIL)
2. Keep lock only in file watcher `_reload_runners()` for write (though even this is technically optional)
3. Add heartbeat logging to backend.log file for debugging

```python
# Before (blocks event loop):
async def get_cached_watchlist_async(...):
    with _cache_lock:  # BLOCKS EVENT LOOP!
        return _cached_watchlist

# After (no lock needed for reads):
async def get_cached_watchlist_async(...):
    # Simple read is atomic in Python (GIL)
    return _cached_watchlist
```

### Fix 13: IPv6 Connection Hang on Windows
**Files**: `backend/services/file_watcher.py`, `backend/services/quote_relay.py`, `backend/services/llm_validator.py`
**Risk**: LOW

**Problem**: Backend still hangs despite all previous fixes. netstat shows `SYN_SENT` to `[::1]:8080` (IPv6) stuck indefinitely while trader app responds fine on `127.0.0.1` (IPv4).

**Root cause**: httpx uses `localhost` which resolves to both IPv4 (`127.0.0.1`) and IPv6 (`::1`). On Windows:
1. Python's socket prefers IPv6 when available
2. If trader app only binds to `0.0.0.0` (IPv4), IPv6 connections hang
3. SYN packet sent to `::1:8080` never gets RST or SYN-ACK
4. httpx's connect timeout may not fire correctly for stuck SYN
5. Event loop freezes waiting for connection

**Evidence**: `netstat -ano | findstr PID` showed:
```
TCP    [::1]:54022            [::1]:8080             SYN_SENT        1352616
```
While curl to `http://localhost:8080` worked fine (resolved to IPv4).

**Fix**: Replace all `localhost` with `127.0.0.1` to force IPv4:
```python
# file_watcher.py
api_url = _trader_api_url.replace("localhost", "127.0.0.1")

# quote_relay.py
self.trader_url = trader_url.replace("localhost", "127.0.0.1")

# llm_validator.py
base_url = base_url.replace("localhost", "127.0.0.1")
```

### Fix 14: Diagnostic Logging for Hang Point Identification
**Files**: `backend/services/file_watcher.py`
**Commit**: `73ac018`
**Risk**: LOW

**Problem**: Backend still hangs after Fix #13 (IPv6). Need granular logging to identify exact freeze point.

**Approach**: Added step-by-step logging in `fetch_watchlist_from_trader_async()`:
- Before request starts
- After client created
- After response received
- After JSON parsed
- After exiting async context
- After cache update
- Completed successfully

**Key finding**: Log showed `fetch_watchlist_from_trader_async: completed successfully` but `get_cached_watchlist_async: fetch completed` never appeared. The only code between these two log lines was... nothing useful. But the file still had `print()` statements elsewhere.

### Fix 15: Replace All print() with Logging (DEFINITIVE FIX)
**Files**: `backend/main.py`, `backend/services/file_watcher.py`, `backend/services/quote_relay.py`, `backend/services/schwab_client.py`
**Commit**: `c715391`
**Risk**: LOW

**Problem**: Backend hung repeatedly despite all previous fixes. The diagnostic logging (Fix #14) revealed the freeze happened after async operations completed, not during them.

**Root cause**: `print()` to stdout can block the asyncio event loop on Windows:
1. Console stdout is line-buffered
2. If console buffer is full or in specific states (minimized, hidden), `print()` blocks
3. The heartbeat loop was calling `print(msg)` every 30 seconds
4. When stdout blocked, the heartbeat task blocked the event loop
5. All async operations froze

**Evidence**: After Fix #14, logs showed all async operations completing successfully, but function returns were freezing. The only code between "completed successfully" log and next log point was trivial - the issue was in how Python was flushing output.

**Fix**: Replaced ALL `print()` statements in backend with `_logger.info/warning/error()`:
```python
# Before (BLOCKS event loop on Windows):
print(f"[HEARTBEAT] #{count} - Event loop alive, {pending} tasks")

# After (writes to file, never blocks):
_logger.info(f"[HEARTBEAT] #{count} - Event loop alive, {pending} tasks")
```

**Files cleaned**:
- `backend/main.py`: 13 print() → _logger.info()
- `backend/services/file_watcher.py`: 5 print() → _logger.info/error()
- `backend/services/quote_relay.py`: 5 print() → _logger.info/debug()
- `backend/services/schwab_client.py`: 1 print() → _logger.info()

## Git Commits (Today)

| Hash | Message |
|------|---------|
| `c715391` | fix: Replace all print() with logging in backend |
| `9004910` | fix: Add diagnostic logging for watchlist fetch |
| `73ac018` | debug: Add granular logging for hang diagnosis |
| `24416e9` | fix: Force IPv4 to avoid IPv6 connection hangs on Windows |
| `c5b7ca8` | fix: Remove threading.Lock from async code paths |
| `5aafbd0` | fix: Fresh httpx client per request to prevent pool corruption |
| `1cc5063` | fix: Remove asyncio.Lock deadlock from httpx client management |
| `3ad3caf` | fix: Improve launcher cleanup to kill hung backend processes |
| `914e297` | fix: Replace sync httpx client with async client in file_watcher.py |
| `46b5770` | fix: Frontend crash prevention and chart error handling |
| `0dc9a5d` | fix: Thread pool exhaustion and timeout wrappers for async calls |

## Next Steps
- Monitor for stability after Fix #15 (print→logging)
- If stable for 1+ hour, the backend hang saga is resolved
- Consider audio alert for D1 High breakout (optional, could be annoying)
- Long-term stability test with streaming enabled

---

**Last Updated**: 2026-01-30
